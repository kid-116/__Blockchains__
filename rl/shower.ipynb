{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discrete(3).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kid-116/.local/lib/python3.8/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([47.028362], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(low=np.array([0]), high=np.array([100])).sample()\n",
    "Box(0, 100, shape=(1, )).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, array([0.54304576, 0.614665  , 0.68051237], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuple((Discrete(3), Box(0, 1, shape=(3,)))).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('height', 0)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dict({'height': Discrete(2)}).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0], dtype=int8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiBinary(4).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiDiscrete([3, 6, 2]).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env\n",
    "- Build an agent to give best shower\n",
    "- Temp changes randomly?\n",
    "- Optimum 37-39\n",
    "- Optimum score will be 90% of shower_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Shower-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.state += action-1\n",
    "        \n",
    "        self.shower_length -= 1\n",
    "\n",
    "        if self.state >= 37 and self.state <= 39:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        done = True if self.shower_length <= 0 else False\n",
    "\n",
    "        if self.state < 0 or self.state > 100:\n",
    "            reward = -100\n",
    "            done = True\n",
    "            self.state = 0 if self.state < 0 else 100\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.array([38 + random.randint(-3, 3)]).astype(float) # initial state\n",
    "        self.shower_length = 60\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep #1 - Score: -46\n",
      "Ep #2 - Score: -20\n",
      "Ep #3 - Score: 16\n",
      "Ep #4 - Score: -58\n",
      "Ep #5 - Score: -60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kid-116/.local/lib/python3.8/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env = ShowerEnv()\n",
    "\n",
    "episodes = 5\n",
    "for ep in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    \n",
    "    print(f\"Ep #{ep} - Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Stack of Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ShowerEnv()\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = os.path.join('training', 'logs', ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# new\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "MODEL_PATH = os.path.join('training', 'saved_models', ENV_NAME, 'v1')\n",
    "model = PPO.load(MODEL_PATH, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to training/logs/Shower-v0/PPO_4\n",
      "Eval num_timesteps=1000, episode_reward=-12.00 +/- 58.79\n",
      "Episode length: 60.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 60       |\n",
      "|    mean_reward     | -12      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-12.00 +/- 58.79\n",
      "Episode length: 60.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 60       |\n",
      "|    mean_reward     | -12      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1890 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=36.00 +/- 48.00\n",
      "Episode length: 60.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 60          |\n",
      "|    mean_reward          | 36          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008365981 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.0232     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.8        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 60.7        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=60.00 +/- 0.00\n",
      "Episode length: 60.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 60       |\n",
      "|    mean_reward     | 60       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 60.00  is above the threshold 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7ff5a395eca0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=50, verbose=1)\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    env,\n",
    "    callback_on_new_best=stop_callback,\n",
    "    eval_freq=1000,\n",
    "    best_model_save_path=os.path.join('training', 'saved_models', ENV_NAME, input()),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to training/logs/Shower-v0/PPO_5\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2756 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1877        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004458856 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.000411    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    value_loss           | 47.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1769        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012465639 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.00025     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1714         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0105793355 |\n",
      "|    clip_fraction        | 0.214        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000444     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.2         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0189      |\n",
      "|    value_loss           | 50.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1686        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008948528 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.00146     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 37.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1682        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008765379 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.00173     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.000767   |\n",
      "|    value_loss           | 35.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1673       |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01565092 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | -0.00222   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.7       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00507   |\n",
      "|    value_loss           | 28.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1670        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012581552 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 2.94e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.00236     |\n",
      "|    value_loss           | 35.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1672        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012349736 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.00255     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1672         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051544877 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.00286      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -7.91e-05    |\n",
      "|    value_loss           | 31.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1667        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011378279 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.0011     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.3        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00632    |\n",
      "|    value_loss           | 41          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1664       |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01530054 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.000173   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25.3       |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.00115   |\n",
      "|    value_loss           | 48.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1666        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007941298 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -5.57e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.00463     |\n",
      "|    value_loss           | 50.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1658        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008351631 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.997      |\n",
      "|    explained_variance   | -7.62e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.2        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.00381     |\n",
      "|    value_loss           | 58.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1652        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008453034 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | -1.75e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.00472     |\n",
      "|    value_loss           | 66.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1648         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062014516 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.991       |\n",
      "|    explained_variance   | 2.21e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.5         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | 0.000494     |\n",
      "|    value_loss           | 64.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1636        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009359887 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | 1.6e-05     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.00573     |\n",
      "|    value_loss           | 68.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1632        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016537478 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.986      |\n",
      "|    explained_variance   | -1.91e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    value_loss           | 74.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1626        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006915219 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.984      |\n",
      "|    explained_variance   | -6.56e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42          |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | 0.00867     |\n",
      "|    value_loss           | 75.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1624        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015847813 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | -2.74e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.2        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.00804     |\n",
      "|    value_loss           | 82.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7ff59af24370>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "SAVE_PATH = os.path.join('training', 'saved_models', ENV_NAME, 'v1')\n",
    "model.save(SAVE_PATH)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.52, 0.854166260162505)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=100, render=False)\n",
    "# returns reward, std deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep #1 - Score: [44.]\n",
      "Ep #2 - Score: [58.]\n",
      "Ep #3 - Score: [30.]\n",
      "Ep #4 - Score: [46.]\n",
      "Ep #5 - Score: [48.]\n",
      "Ep #6 - Score: [48.]\n",
      "Ep #7 - Score: [30.]\n",
      "Ep #8 - Score: [52.]\n",
      "Ep #9 - Score: [34.]\n",
      "Ep #10 - Score: [42.]\n",
      "Ep #11 - Score: [46.]\n",
      "Ep #12 - Score: [40.]\n",
      "Ep #13 - Score: [58.]\n",
      "Ep #14 - Score: [28.]\n",
      "Ep #15 - Score: [50.]\n",
      "Ep #16 - Score: [54.]\n",
      "Ep #17 - Score: [48.]\n",
      "Ep #18 - Score: [52.]\n",
      "Ep #19 - Score: [46.]\n",
      "Ep #20 - Score: [54.]\n",
      "Ep #21 - Score: [50.]\n",
      "Ep #22 - Score: [46.]\n",
      "Ep #23 - Score: [54.]\n",
      "Ep #24 - Score: [52.]\n",
      "Ep #25 - Score: [56.]\n",
      "Ep #26 - Score: [54.]\n",
      "Ep #27 - Score: [44.]\n",
      "Ep #28 - Score: [54.]\n",
      "Ep #29 - Score: [40.]\n",
      "Ep #30 - Score: [54.]\n",
      "Ep #31 - Score: [46.]\n",
      "Ep #32 - Score: [42.]\n",
      "Ep #33 - Score: [34.]\n",
      "Ep #34 - Score: [56.]\n",
      "Ep #35 - Score: [46.]\n",
      "Ep #36 - Score: [46.]\n",
      "Ep #37 - Score: [50.]\n",
      "Ep #38 - Score: [56.]\n",
      "Ep #39 - Score: [48.]\n",
      "Ep #40 - Score: [34.]\n",
      "Ep #41 - Score: [46.]\n",
      "Ep #42 - Score: [48.]\n",
      "Ep #43 - Score: [40.]\n",
      "Ep #44 - Score: [40.]\n",
      "Ep #45 - Score: [30.]\n",
      "Ep #46 - Score: [52.]\n",
      "Ep #47 - Score: [48.]\n",
      "Ep #48 - Score: [42.]\n",
      "Ep #49 - Score: [34.]\n",
      "Ep #50 - Score: [48.]\n",
      "Ep #51 - Score: [32.]\n",
      "Ep #52 - Score: [52.]\n",
      "Ep #53 - Score: [42.]\n",
      "Ep #54 - Score: [24.]\n",
      "Ep #55 - Score: [50.]\n",
      "Ep #56 - Score: [48.]\n",
      "Ep #57 - Score: [36.]\n",
      "Ep #58 - Score: [34.]\n",
      "Ep #59 - Score: [34.]\n",
      "Ep #60 - Score: [56.]\n",
      "Ep #61 - Score: [44.]\n",
      "Ep #62 - Score: [46.]\n",
      "Ep #63 - Score: [44.]\n",
      "Ep #64 - Score: [32.]\n",
      "Ep #65 - Score: [52.]\n",
      "Ep #66 - Score: [52.]\n",
      "Ep #67 - Score: [46.]\n",
      "Ep #68 - Score: [48.]\n",
      "Ep #69 - Score: [44.]\n",
      "Ep #70 - Score: [30.]\n",
      "Ep #71 - Score: [42.]\n",
      "Ep #72 - Score: [52.]\n",
      "Ep #73 - Score: [52.]\n",
      "Ep #74 - Score: [38.]\n",
      "Ep #75 - Score: [40.]\n",
      "Ep #76 - Score: [54.]\n",
      "Ep #77 - Score: [50.]\n",
      "Ep #78 - Score: [42.]\n",
      "Ep #79 - Score: [30.]\n",
      "Ep #80 - Score: [44.]\n",
      "Ep #81 - Score: [40.]\n",
      "Ep #82 - Score: [50.]\n",
      "Ep #83 - Score: [52.]\n",
      "Ep #84 - Score: [42.]\n",
      "Ep #85 - Score: [30.]\n",
      "Ep #86 - Score: [44.]\n",
      "Ep #87 - Score: [34.]\n",
      "Ep #88 - Score: [38.]\n",
      "Ep #89 - Score: [36.]\n",
      "Ep #90 - Score: [54.]\n",
      "Ep #91 - Score: [54.]\n",
      "Ep #92 - Score: [52.]\n",
      "Ep #93 - Score: [46.]\n",
      "Ep #94 - Score: [48.]\n",
      "Ep #95 - Score: [52.]\n",
      "Ep #96 - Score: [30.]\n",
      "Ep #97 - Score: [48.]\n",
      "Ep #98 - Score: [50.]\n",
      "Ep #99 - Score: [50.]\n",
      "Ep #100 - Score: [50.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "for ep in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    \n",
    "    print(f\"Ep #{ep} - Score: {score}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "945d9b4b4e821cdfb190d8b9764f998578a4b0e911ae9f184bb6b843a583816f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
